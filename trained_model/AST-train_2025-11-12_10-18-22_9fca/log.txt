#============================================ AST TRAINING CONFIG ===========================================#
timestamp                     : 2025-11-12T10:18:25
action_sampling_period        : 1200
batch_size                    : 256
buffer_size                   : 1000000
device                        : cuda
engine_step_count             : 10
ent_coef                      : auto
gamma                         : 0.99
gradient_steps                : 1
learning_rate                 : 0.0003
learning_starts               : 25000
lookahead_distance            : 1000
map_gpkg_filename             : Stangvik.gpkg
max_sea_state                 : SS 5
nav_fail_time                 : 600
radius_of_acceptance          : 300
seed                          : None
ship_draw                     : True
stats_window_size             : 25
target_entropy                : auto
target_update_interval        : 1
tau                           : 0.005
tensorboard_log               : True
time_since_last_ship_drawing  : 30
time_step                     : 5
total_steps                   : 100000
train_freq                    : 1
verbose                       : 1
warm_up_time                  : 2000
#----------------------------------- Used AST Environment Wrapper -------------------------------------#
env id/name                   : SeaEnvASTv2
#============================================ RL TRANSITION ===========================================#
#--------------------------------------------- Observation --------------------------------------------#
north                  [m] : [9077090.000 9080618.000 9080660.000 9076663.000 9071660.000 9071053.000]
east                   [m] : [940555.500 936666.875 932831.562 931502.875 930152.000 929576.812]
heading              [deg] : [ -43.016  -86.113 -180.000 -155.311 -130.141  -96.588]
speed                [m/s] : [4.519 4.315 4.742 4.463 4.509 4.057]
cross track error      [m] : [86.545  0.000 82.560 94.102  0.000  0.000]
wind speed           [m/s] : [ 4.426 12.805 13.223 12.815 12.785 12.338]
wind dir             [deg] : [-180.000  180.000  180.000  180.000  180.000  180.000]
current speed        [m/s] : [0.169 0.754 0.979 0.747 0.951 0.773]
current dir          [deg] : [ 180.000 -180.000 -180.000  180.000  180.000  180.000]
#----------------------------------------------- Action -----------------------------------------------#
sampling timestamp     [s] : [2000.000 3200.000 4400.000 5600.000 6800.000]
Hs                     [m] : [3.284 3.304 3.282 3.350 3.363]
U_w_bar              [m/s] : [12.797 13.138 12.724 12.568 12.621]
U_w_bar             [knot] : [24.874 25.538 24.734 24.431 24.534]
Tp                     [s] : [10.030 10.946 11.214 11.527 11.158]
psi_ww_bar           [deg] : [ 28.196 134.715 110.832  91.988  54.455]
U_c_bar              [m/s] : [0.725 0.967 0.708 0.919 0.843]
psi_c_bar            [deg] : [ -54.867 -150.593   29.457  166.459  157.430]
action validity            : [ True  True  True  True  True]
sea state                  : ['SS 5' 'SS 5' 'SS 5' 'SS 5' 'SS 5']
#------------------------------------------------------------------------------------------------------#
Terminated                 : [False False False False  True]
#------------------------------------------------------------------------------------------------------#
Truncated                  : [False False False False False]
#------------------------------------------------------------------------------------------------------#
Total Reward               : [ -6.420  -7.913  -6.425  -5.121 -12.536]
Base Reward                : [0.000 1.000 2.000 3.000 4.000]
Sea state log-prob         : [-3.061 -3.551 -3.277 -3.369 -3.289]
current speed log-prob     : [-0.668 -0.831 -0.915 -0.820 -0.881]
current direction log-prob : [-1.498 -2.174 -3.074 -2.805 -1.086]
wind direction log-prob    : [-1.192 -2.358 -1.159 -1.127 -1.281]
#------------------------------------------------------------------------------------------------------#
Training is done in 3 hours, 45 minutes, and 22 seconds.

Reward Function:
def reward_function(self, action, logp_floor=-50.0, eta=0.5, theta=2.0):
        """
        For this reward function, we only take into account the own_ship
        Param:
        logp_floor  : clip value when encountering log prob value of -inf
        eta         : coefficient to aim for immediate or late failure discovery. Range: [0(immediate), 1(late)]
        theta       : gain value to encourage immediate failure discovery
        """
        ## Unpack action
        [Hs, U_w_bar, Tp, psi_ww_bar, U_c_bar, psi_c_bar] = action
        
        ## Base reward -> Encourage further exploration
        base_reward = len(self.action_list) * eta * theta
        reward = base_reward
        
        ## Get the termination info of the own ship
        collision           = self.assets[0].ship_model.stop_info['collision']
        grounding_failure   = self.assets[0].ship_model.stop_info['grounding_failure']
        navigation_failure  = self.assets[0].ship_model.stop_info['navigation_failure']
        reaches_endpoint    = self.assets[0].ship_model.stop_info['reaches_endpoint']
        outside_horizon     = self.assets[0].ship_model.stop_info['outside_horizon']
        power_overload      = self.assets[0].ship_model.stop_info['power_overload']
        
        ## Get reward from the environmental load log probability
        # Sea state marginal log likelihood (clip to floor if we encounter log prob of negative infinity)
        sea_state_ll            = max(self.sea_state_mixture.logpdf_marginal(Hs, U_w_bar, Tp), logp_floor)
        
        # Current speed direction (clip to floor if we encounter log prob of negative infinity)
        current_speed_ll        = max(logprior_mu_speed(U_c_bar, range=(self.current_range["min"][0], self.current_range["max"][0]), center=self.U_c_bar_prev, sigma_frac=0.25),
                                  logp_floor)
        
        # Current speed direction
        current_direction_ll    = logprior_mu_direction(psi_c_bar, clim_mean_dir=self.psi_c_bar_prev, kappa0=1.0)
        
        # Wind speed direction
        wind_direction_ll       = logprior_mu_direction(psi_ww_bar, clim_mean_dir=self.psi_ww_bar_prev, kappa0=1.0)
        
        # Sum all the log likelihood to get the reward_signal
        reward_env_ll = sea_state_ll + current_speed_ll + current_direction_ll + wind_direction_ll 
        
        # Add to the base reward
        reward += reward_env_ll
        
        # Update list
        self.base_reward_list.append(base_reward)
        self.sea_state_ll_list.append(sea_state_ll)
        self.current_speed_ll_list.append(current_speed_ll)
        self.current_direction_ll_list.append(current_direction_ll)
        self.wind_direction_ll_list.append(wind_direction_ll)
        
        ## Get reward from termination status
        if outside_horizon:
            reward += -5.0      # We only want grounding failure
        elif collision or power_overload or navigation_failure:
            reward += 5.0       # Not the main focus, but are welcomed
        elif grounding_failure:
            reward += 15.0      # We focus on finding grounding failure
        elif reaches_endpoint:
            reward += -10.0     # We highly discourage the agent to let the ship finishes its mission.
        
        return reward